{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-time Personalization with Latent Factor Models\n",
    "-------------------------------------------------\n",
    "\n",
    "## Overview\n",
    "Latent factor models (i.e. - collaborative filtering) can provide a powerful abstraction for creating usable user-item preference data. By setting up the desired product affinity matrix, various matrix factorization techniques can be used to find a lower rank approximation of this matrix when given (as is usually the case) a small known / measured portion of the full matrix. The final user and item matrices (of rank R) have also been shown to be quite useful in other derivative personalization techniques.\n",
    "\n",
    "## Notebook Overview\n",
    "Below is a walkthrough from start to finish of a method for finding the top personalized recommendations for films similar to [Con Air](http://www.imdb.com/title/tt0118880/). We will use the [MovieLens](https://grouplens.org/datasets/movielens/) data set and find a very simple low-rank approximation of the user-movie affinity matrix. We will also show how this model can be tailored with real-time affinities that can personalize the recommendations even further without needing to re-compute the entire user-item affinity matrix.\n",
    "\n",
    "1. Load and process MovieLens data\n",
    "2. Determine best hyper-parameters for ALS model\n",
    "3. Train the full model\n",
    "4. Find similar movies to a target film based on latent user preferences\n",
    "5. Derive real-time method for ranking films\n",
    "6. Test examples of real-time recommendations\n",
    "\n",
    "## Supporting Material\n",
    "1. Slide [presentation](http://slides.com/dataexhaust/deck-c32b28f0-f834-45fb-9143-2362f74050ab#/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (io.netty.util.internal.logging.InternalLoggerFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$exclude.$                        , $ivy.$                            // for cleaner logs\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$profile.$           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // adjust spark version - spark >= 2.0\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
       "\n",
       "// General spark imports\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjupyter.spark.session._\n",
       "\n",
       "// Create sessions\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@2935f3af\n",
       "\u001b[36msc\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@2c47648a\n",
       "\u001b[36msqlContext\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mSQLContext\u001b[39m = org.apache.spark.sql.SQLContext@4597f178"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    " *  Environment Setup\n",
    " *  ========================\n",
    " *  - Jupyter-Scala (https://github.com/alexarchambault/jupyter-scala)\n",
    " */\n",
    "import $exclude.`org.slf4j:slf4j-log4j12`, $ivy.`org.slf4j:slf4j-nop:1.7.21` // for cleaner logs\n",
    "import $profile.`hadoop-2.6`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.1.0` // adjust spark version - spark >= 2.0\n",
    "import $ivy.`org.apache.hadoop:hadoop-aws:2.6.4`\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.0` // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
    "\n",
    "// General spark imports\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import jupyter.spark.session._\n",
    "\n",
    "// Create sessions\n",
    "val spark = JupyterSparkSession.builder() // important - call this rather than SparkSession.builder()\n",
    "  .jupyter() // this method must be called straightaway after builder()\n",
    "  // .yarn(\"/etc/hadoop/conf\") // optional, for Spark on YARN - argument is the Hadoop conf directory\n",
    "  // .emr(\"2.6.4\") // on AWS ElasticMapReduce, this adds aws-related to the spark jar list\n",
    "  .master(\"local[*]\") // change to \"yarn-client\" on YARN\n",
    "  .config(\"spark.driver.memory\", \"8g\")\n",
    "  .config(\"spark.executor.memory\", \"8g\")\n",
    "  .appName(\"Graph-based Review Summarization\")\n",
    "  .getOrCreate()\n",
    "\n",
    "// Access underlying spark context (for backwards compatibility)\n",
    "val sc = spark.sparkContext\n",
    "val sqlContext = spark.sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://repo1.maven.org/maven2/com/databricks/spark-csv_2.11/1.5.0/spark-csv_2.11-1.5.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/databricks/spark-csv_2.11/1.5.0/spark-csv_2.11-1.5.0.pom.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/com/databricks/spark-csv_2.11/1.5.0/spark-csv_2.11-1.5.0.pom.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/com/databricks/spark-csv_2.11/1.5.0/spark-csv_2.11-1.5.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/commons/commons-csv/1.1/commons-csv-1.1.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/commons/commons-csv/1.1/commons-csv-1.1.pom.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/commons/commons-csv/1.1/commons-csv-1.1.pom.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/commons/commons-csv/1.1/commons-csv-1.1.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/commons/commons-csv/1.1/commons-csv-1.1.jar.sha1\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/commons/commons-csv/1.1/commons-csv-1.1.jar\n",
      "Downloading https://repo1.maven.org/maven2/com/databricks/spark-csv_2.11/1.5.0/spark-csv_2.11-1.5.0.jar\n",
      "Downloading https://repo1.maven.org/maven2/com/databricks/spark-csv_2.11/1.5.0/spark-csv_2.11-1.5.0.jar.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/commons/commons-csv/1.1/commons-csv-1.1.jar.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/commons/commons-csv/1.1/commons-csv-1.1.jar\n",
      "Downloaded https://repo1.maven.org/maven2/com/databricks/spark-csv_2.11/1.5.0/spark-csv_2.11-1.5.0.jar.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/com/databricks/spark-csv_2.11/1.5.0/spark-csv_2.11-1.5.0.jar\n"
     ]
    }
   ],
   "source": [
    "// Load other related dependencies\n",
    "interp.load.ivy(\"org.scalanlp\" %% \"breeze\" % \"0.13\")\n",
    "interp.load.ivy(\"org.apache.spark\" %% \"spark-mllib\" % \"2.0.2\")\n",
    "interp.load.ivy(\"com.databricks\" %% \"spark-csv\" % \"1.5.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36msqlContext._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36msqlContext.implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.rdd._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkContext._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\n",
       "// ML Imports\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mbreeze.linalg._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.linalg.Vectors\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.recommendation.ALS\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.recommendation.MatrixFactorizationModel\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.recommendation.Rating\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Spark libraries\n",
    "import sqlContext._\n",
    "import sqlContext.implicits._\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.rdd._\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "// ML Imports\n",
    "import breeze.linalg._\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.recommendation.ALS\n",
    "import org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n",
    "import org.apache.spark.mllib.recommendation.Rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support methods\n",
    "These are a couple of support methods for later derivations - in particular CosineSimilarity and a Matrix creation helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmatrix\u001b[39m\n",
       "defined \u001b[32mobject\u001b[39m \u001b[36mCosineSimilarity\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    "* Helper Matrix Generation\n",
    " * ====\n",
    " * Creates and populates a 2D Breeze matrix from array of arrays\n",
    " */\n",
    "def matrix(data: Array[Array[Double]], rows: Int, cols: Int): DenseMatrix[Double] = {\n",
    "  // Allocate an array of arrays\n",
    "  val matrix = DenseMatrix.zeros[Double](rows, cols)\n",
    "\n",
    "  // Iterate over each position (i,j) in source matrix\n",
    "  (0 to (data.length - 1)).foreach(i => {\n",
    "    (0 to (data(0).length - 1)).foreach(j => {\n",
    "      matrix(i, j) = data(i)(j)\n",
    "    })\n",
    "  })\n",
    "\n",
    "  // Return DenseMatrix\n",
    "  matrix\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "/*\n",
    " * Object in scala for calculating cosine similarity\n",
    " * Reuben Sutton - 2012\n",
    " * More information: http://en.wikipedia.org/wiki/Cosine_similarity\n",
    " */\n",
    "\n",
    "object CosineSimilarity {\n",
    "\n",
    "  /*\n",
    "   * This method takes 2 equal length arrays of integers\n",
    "   * It returns a double representing similarity of the 2 arrays\n",
    "   * 0.9925 would be 99.25% similar\n",
    "   * (x dot y)/||X|| ||Y||\n",
    "   */\n",
    "  def cosineSimilarity(x: Array[Double], y: Array[Double]): Double = {\n",
    "    require(x.size == y.size)\n",
    "    dotProduct(x, y)/(magnitude(x) * magnitude(y))\n",
    "  }\n",
    "\n",
    "  /*\n",
    "   * Return the dot product of the 2 arrays\n",
    "   * e.g. (a[0]*b[0])+(a[1]*a[2])\n",
    "   */\n",
    "  def dotProduct(x: Array[Double], y: Array[Double]): Double = {\n",
    "    (for((a, b) <- x zip y) yield a * b) sum\n",
    "  }\n",
    "\n",
    "  /*\n",
    "   * Return the magnitude of an array\n",
    "   * We multiply each element, sum it, then square root the result.\n",
    "   */\n",
    "  def magnitude(x: Array[Double]): Double = {\n",
    "    math.sqrt(x map(i => i*i) sum)\n",
    "  }\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load movie data\n",
    "MovieLens ratings come in a very simple form:\n",
    "\n",
    "```\n",
    "userId,movieId,rating,timestamp\n",
    "1,169,2.5,1204927694\n",
    "1,2471,3.0,1204927438\n",
    "1,48516,5.0,1204927435\n",
    "2,2571,3.5,1436165433\n",
    "2,109487,4.0,1436165496\n",
    "2,112552,5.0,1436165496\n",
    "2,112556,4.0,1436165499\n",
    "3,356,4.0,920587155\n",
    "3,2394,4.0,920586920\n",
    "3,2431,5.0,920586945\n",
    "3,2445,4.0,920586945\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Load ratings data -- using Spark CSV (https://github.com/databricks/spark-csv)\n",
    "val ratings = sqlContext.read.format(\"com.databricks.spark.csv\")\n",
    "                        .option(\"header\", \"true\").option(\"inferSchema\", \"true\")\n",
    "                        .load(\"file:///.../movies/ml-latest/ratings.csv\")\n",
    "                        .map(r => {\n",
    "                            Rating(r(0).toString.toInt, r(1).toString.toInt, r(2).toString.toDouble)\n",
    "                        }).registerTempTable(\"ratings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine hyper-parameters\n",
    "Since we using the Alternating-Least Squares algorithm (comes standard in SparkML), it requires a couple of hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Create training, validation, and test datasets\n",
    "val Array(training, test) = ratings.randomSplit(Array(0.8, 0.2))\n",
    "\n",
    "// Iterate over models and train\n",
    "def trainAndTest(rank: Int, lambda: Double, numIter: Int) = {\n",
    "    // Build the recommendation model using ALS on the training data\n",
    "    val model = ALS.train(training, rank, numIter, lambda)\n",
    "\n",
    "    // Evaluate the model on test data\n",
    "    val usersProducts = test.map { case Rating(user, product, rate) =>\n",
    "        (user, product)\n",
    "    }\n",
    "    val predictions = model.predict(usersProducts).map { case Rating(user, product, rate) =>\n",
    "        ((user, product), rate)\n",
    "    }\n",
    "\n",
    "    val ratesAndPreds = test.map { case Rating(user, product, rate) =>\n",
    "        ((user, product), rate)\n",
    "    }.join(predictions)\n",
    "\n",
    "    val MSE = ratesAndPreds.map { case ((user, product), (r1, r2)) =>\n",
    "        val err = (r1 - r2)\n",
    "        err * err\n",
    "    }.mean()\n",
    "\n",
    "    println(\"RMSE (validation) = \" + MSE + \" for the model trained with rank = \" \n",
    "                                   + rank + \", lambda = \" + lambda \n",
    "                                   + \", and numIter = \" + numIter + \".\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on full data-set (once hyper-parameters selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/* ==== Test Results ====\n",
    "    [Baseline]\n",
    "    RMSE (validation) = 0.6832349868066907 for the model\n",
    "            trained with rank = 5, lambda = 0.1, and numIter = 5.\n",
    "\n",
    "    [Pivot on Rank]\n",
    "    RMSE (validation) = 0.6800594873467324 for the model \n",
    "            trained with rank = 10, lambda = 0.1, and numIter = 5.\n",
    "    RMSE (validation) = 0.690144394236897 for the model \n",
    "            trained with rank = 20, lambda = 0.1, and numIter = 5.\n",
    "    RMSE (validation) = 0.698129529945344 for the model \n",
    "            trained with rank = 50, lambda = 0.1, and numIter = 5.\n",
    "\n",
    "    [Pivot on Lambda]\n",
    "    RMSE (validation) = 0.6925651848679597 for the model \n",
    "            trained with rank = 5, lambda = 0.01, and numIter = 5.\n",
    "    RMSE (validation) = 1.0692672408983346 for the model \n",
    "            trained with rank = 5, lambda = 0.5, and numIter = 5.\n",
    "    RMSE (validation) = 1.7497220606946313 for the model \n",
    "            trained with rank = 5, lambda = 1.0, and numIter = 5.\n",
    "\n",
    "    [Use Best Performing Parameters]\n",
    "    RMSE (validation) = 0.6649058015762571 for the model \n",
    "            trained with rank = 10, lambda = 0.1, and numIter = 20.\n",
    " */\n",
    "\n",
    "// Train full model\n",
    "val model = ALS.train(ratings, 10, 10, 0.1)\n",
    "\n",
    "// Save model\n",
    "model.save(sc, \"file:///.../movies/models/ml/v1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find similar movies to Con-Air\n",
    "With a trained latent-factor model, we can use it to find similar movies to a target film (Con Air) based simply on the implicit preferences of people who have like Con Air and movies with similar people to those that like Con Air."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Open raw movie data\n",
    "val movies = sqlContext.read.format(\"com.databricks.spark.csv\")\n",
    "                       .option(\"header\", \"true\").option(\"inferSchema\", \"true\")\n",
    "                       .load(\"file:///.../movies/ml-latest/movies.csv\")\n",
    "                       .registerTempTable(\"movies\")\n",
    "\n",
    "// Load item factors\n",
    "case class ItemFactor(id: Int, features: Array[Double])\n",
    "case class IndexedItemFactor(id: Int, features: Array[Double], index: Int)\n",
    "val item_factors = sqlContext.load(\"file:///.../movies/models/ml/v1/data/product\",\"parquet\")\n",
    "                             .as[ItemFactor].rdd.zipWithIndex.map(x => {\n",
    "                                IndexedItemFactor(x._1.id, x._1.features, x._2.toInt)\n",
    "                             })\n",
    "item_factors.toDF().registerTempTable(\"item_factors\")\n",
    "\n",
    "// Get the feature vector for Con Air\n",
    "val target_item_factors = item_factors.filter(i => i.id == 1552).take(1)(0).features\n",
    "\n",
    "// Iterate over all other item factors and map a cosine distance\n",
    "case class SimilarItem(movie_id: Int, similarity: Double)\n",
    "val similar_items = sc.parallelize(item_factors.collect()).map(i => {\n",
    "    // Calculate similarity\n",
    "    val similarity = CosineSimilarity.cosineSimilarity(target_item_factors, i.features)\n",
    "\n",
    "    (similarity, i)\n",
    "}).takeOrdered(20)(Ordering[Double].reverse.on(x=>x._1)).map(x => SimilarItem(x._2.id, x._1))\n",
    "\n",
    "// Prep for Spark SQL\n",
    "sc.parallelize(similar_items).toDF().registerTempTable(\"similar_items\")\n",
    "\n",
    "// Merge with book lookup and show top 10 similar books\n",
    "sqlContext.sql(\"SELECT m.title, s.similarity, s.rank \n",
    "                FROM movies m JOIN similar_items s ON m.movieId = s.movie_id \n",
    "                ORDER BY s.similarity DESC LIMIT 20\").foreach(println)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "[Con Air (1997),1.0]\n",
    "[Bad Boys (1995),0.9885250995229632]\n",
    "[Striking Distance (1993),0.9868059080297423]\n",
    "[City Hunter (Sing si lip yan) (1993),0.9857155838604543]\n",
    "[Rock, The (1996),0.9855154341766271]\n",
    "[Another 48 Hrs. (1990),0.9842966650016308]\n",
    "[Program, The (1993),0.9830120663020663]\n",
    "[Young Guns II (1990),0.982577633206861]\n",
    "[Face/Off (1997),0.9825008152957304]\n",
    "[Navy Seals (1990),0.9823346682235141]\n",
    "[Assassins (1995),0.9820721829300805]\n",
    "[Lethal Weapon 3 (1992),0.9820630838696822]\n",
    "[Sharpe's Challenge (2006),0.9820047546907581]\n",
    "[My Avatar and Me (Min Avatar og mig) (2010),0.9817191629728049]\n",
    "[Blue Streak (1999),0.9817106802763232]\n",
    "[Days of Thunder (1990),0.9816860526998341]\n",
    "[Outbreak (1995),0.9804433142553622]\n",
    "[Whispers in the Dark (1992),0.9803792985641578]\n",
    "[Rapid Fire (1992),0.9803632101298569]\n",
    "[Cowboy Way, The (1994),0.9799338957065918]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting new affinities (in real-time)\n",
    "A simple method for getting a target user's tailored recommendations is to simply measure their own affinities (movie ratings in our case), include them in the whole dataset, and re-run the ALS derivation. This allows one to use the user's specific Preference Vector (P(U)).\n",
    "\n",
    "In the case where we either a) cannot re-calculate the entire latent matrix because recommendations are required in real-time or b) the latent factor matrix is built on different collected dataset than the information available in the recommendation context, we must figure out a more clever way to find an approximation for P(U) given some other measure of known affinity.\n",
    "\n",
    "The derivation for this approach exists in greater detail on my blog [post](http://dataexhaust.io/designing-product-recommendation-engines-for-the-new-age-of-digital-commerce/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Instantiate the user product matrix (in-memory)\n",
    "val number_of_factors = item_factors.first.features.length\n",
    "val number_of_items = item_factors.count().toInt\n",
    "val Y = matrix(item_factors.collect().map(i => i.features), number_of_items, number_of_factors)\n",
    "\n",
    "// Set up preferred films\n",
    "val preferred_movies = Array(\n",
    "  // Provide captured implicit movie preference weights\n",
    ")\n",
    "\n",
    "// Create G = Y * (Y.t * Y) ^ -1\n",
    "val G = Y * inv(Y.t * Y)\n",
    "\n",
    "// Find predicted user factor vector X_u => P_u * Y * (Y.t * Y) ^ -1'\n",
    "val X_u = preferred_movies.map(x => x._3 * G(x._2,::))\n",
    "                          .fold(DenseMatrix.zeros[Double](1, number_of_factors))((acc, v) => {\n",
    "  acc + v\n",
    "})\n",
    "\n",
    "// Use X_u to get personalized movie scores Q_u = X_u * Y.t\n",
    "val Q_u = X_u * Y.t\n",
    "\n",
    "// ..... TEST .......\n",
    "\n",
    "// Sort and pair with movie id\n",
    "case class ItemRating(id: Int, score: Double)\n",
    "val rated_items = (0 to (number_of_items - 1)).map(i => ItemRating(i, Q_u(0, i)))\n",
    "                                              .sortBy(ir => -1.0 * ir.score).toArray\n",
    "sc.parallelize(rated_items).toDF().registerTempTable(\"rated_items\")\n",
    "\n",
    "// Find the top recommended movies\n",
    "sqlContext.sql(\"SELECT m.title, s.similarity, r.score \n",
    "                FROM rated_items r \n",
    "                JOIN movies m ON r.id = m.movieId \n",
    "                JOIN similar_items s ON s.movie_id = r.id \n",
    "                ORDER BY r.score DESC LIMIT 30\").foreach(println)\n",
    "\n",
    "// Taste Profile: \"Lord of War was my JAM!\"\n",
    "val preferred_movies = Array(\n",
    "  (3717, 403, 10.0),       // Gone in Sixty Seconds\n",
    "  (1835, 30170, 50.0),     // City of Angels\n",
    "  (36529, 27388, 100.0),   // Lord of War\n",
    "  (733, 15060, 25.0)       // The Rock\n",
    ")\n",
    "\n",
    "// Taste Profile: \"National Treasure was just the best...\"\n",
    "val preferred_movies = Array(\n",
    "  (3717, 403, 10.0),       // Gone in Sixty Seconds\n",
    "  (1835, 30170, 50.0),     // City of Angels\n",
    "  (8972, 30893, 100.0),    // National Treasure\n",
    "  (47810, 8756, 50.0)      // The Wicker Man\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
