{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing Music Reviews with Graphical Models\n",
    "-------------------------------------------------\n",
    "\n",
    "## Overview\n",
    "Every day, digital shoppers across the globe generate hundreds of thousands of reviews on products - both new and old. As a digital retailer or brand, it is critical to understand not just the sentiment of this feedback but also the core concepts that customers write about; however, the pace of content generation has already outpaced the ability for marketing and merchandising teams at these organizations to read every piece of consumer generated content submitted.\n",
    "\n",
    "\n",
    "Dynamic content summarization techniques can provide a much-needed ability to programmatically identify core concepts within natural text and leverage this insight to condense large amounts of text into information-dense summarizations. In this talk, we will explore the current state-of-the-art in content summary by implementing the graph-based keyword extraction algorithm called TopicRank on music text reviews and then use these extracted concepts to summarize all of the reviews on a given album automatically.\n",
    "\n",
    "## Notebook Overview\n",
    "Below is a walkthrough from start to finish of a method for finding the top 10 most relevant sentences from a corpus of music reviews (in particular - we will be summarizing reviews for Pink Floyd's The Dark Side of the Moon).\n",
    "\n",
    "1. Load raw review data set (music reviews from Amazon - ~1m reviews)\n",
    "2. Find and parse sentences related to Dark Side of the Moon\n",
    "3. Prepare word embeddings from music corpus\n",
    "4. Create a sentence graph\n",
    "5. Compute PageRank on that graph\n",
    "6. Look at a few example results\n",
    "\n",
    "## Supporting Material\n",
    "1. Slide [presentation](http://slides.com/dataexhaust/dynamic-content-summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$exclude.$                        , $ivy.$                            // for cleaner logs\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$profile.$           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // adjust spark version - spark >= 2.0\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
       "\n",
       "// General spark imports\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjupyter.spark.session._\n",
       "\n",
       "// Create sessions\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@73907edd\n",
       "\u001b[36msc\u001b[39m: \u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@d3a7cce\n",
       "\u001b[36msqlContext\u001b[39m: \u001b[32mSQLContext\u001b[39m = org.apache.spark.sql.SQLContext@172150da"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    " *  Environment Setup\n",
    " *  ========================\n",
    " *  - Jupyter-Scala (https://github.com/alexarchambault/jupyter-scala)\n",
    " */\n",
    "import $exclude.`org.slf4j:slf4j-log4j12`, $ivy.`org.slf4j:slf4j-nop:1.7.21` // for cleaner logs\n",
    "import $profile.`hadoop-2.6`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.1.0` // adjust spark version - spark >= 2.0\n",
    "import $ivy.`org.apache.hadoop:hadoop-aws:2.6.4`\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.0` // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
    "\n",
    "// General spark imports\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import jupyter.spark.session._\n",
    "\n",
    "// Create sessions\n",
    "val spark = JupyterSparkSession.builder() // important - call this rather than SparkSession.builder()\n",
    "  .jupyter() // this method must be called straightaway after builder()\n",
    "  // .yarn(\"/etc/hadoop/conf\") // optional, for Spark on YARN - argument is the Hadoop conf directory\n",
    "  // .emr(\"2.6.4\") // on AWS ElasticMapReduce, this adds aws-related to the spark jar list\n",
    "  .master(\"local[*]\") // change to \"yarn-client\" on YARN\n",
    "  .config(\"spark.driver.memory\", \"8g\")\n",
    "  .config(\"spark.executor.memory\", \"8g\")\n",
    "  .appName(\"Graph-based Review Summarization\")\n",
    "  .getOrCreate()\n",
    "\n",
    "// Access underlying spark context (for backwards compatibility)\n",
    "val sc = spark.sparkContext\n",
    "val sqlContext = spark.sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36msqlContext._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36msqlContext.implicits._\n",
       "\n",
       "// ML imports\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mbreeze.linalg._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.linalg.Vectors\n",
       "\n",
       "// Graph imports\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.graphx._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.rdd.RDD\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Dataset\n",
       "\n",
       "// NLP imports\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36medu.stanford.nlp.simple.Document\u001b[39m"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load special ML / NLP libraries via interop\n",
    "interp.load.ivy(\"org.apache.spark\" %% \"spark-mllib\" % \"2.0.2\")\n",
    "interp.load.ivy(\"org.apache.spark\" %% \"spark-graphx\" % \"2.0.2\")\n",
    "interp.load.ivy(\"org.scalanlp\" %% \"breeze\" % \"0.13\")\n",
    "interp.load.ivy(\"edu.stanford.nlp\" % \"stanford-corenlp\" % \"3.6.0\")\n",
    "//interp.load.ivy(\"edu.stanford.nlp\" % \"stanford-corenlp\" % \"3.6.0\" classifier \"models\") -- throws error (https://github.com/alexarchambault/jupyter-scala/issues/128)\n",
    "interp.load.ivy(\"com.google.protobuf\" % \"protobuf-java\" % \"2.6.1\")\n",
    "\n",
    "// Spark SQL\n",
    "import sqlContext._\n",
    "import sqlContext.implicits._\n",
    "\n",
    "// ML imports\n",
    "import breeze.linalg._\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "// Graph imports\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.Dataset\n",
    "\n",
    "// NLP imports\n",
    "import edu.stanford.nlp.simple.Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and parse sentences\n",
    "We will be working with Amazon review data, made available by [UCSD](http://jmcauley.ucsd.edu/data/amazon/). We need to load it into a Spark dataframe, find the reviews related to our target Album (ASIN of B000000IRB), get the raw text and use StanfordNLP parser to get an enriched version of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Load music reviews - find albums with most reviews\n",
    "val music_reviews = sqlContext.load(\"file:///home/garrett/dev/data/amazon/music/reviews_CDs_and_Vinyl_5.json\", \"json\")\n",
    "music_reviews.registerTempTable(\"reviews\")\n",
    "\n",
    "// Create merged review document for target albums\n",
    "val document = sqlContext.sql(\"SELECT reviewText FROM reviews WHERE asin = 'B000000IRB'\").map(r => r(0).toString).collect().mkString(\"\\n\\n\")\n",
    "\n",
    "// Get sentences\n",
    "val sentences = new Document(document).sentences() // 8077 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse out and save necessary keywords\n",
    "We are only interested in using the keywords (nouns and proper nouns) from the sentences, so we need to parse out the relevant text and make a form that will be easier to load from disk later.\n",
    "\n",
    "Note: The below code will not run yet because of the Ivy model loading issue above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Filter out only words that exist in the keywords of sentences that we want to find distance\n",
    "val parsed_sentences = sentences.map(s => {\n",
    "    // Extract words and their part of speech\n",
    "    val words = s.words().toList\n",
    "    val tags = s.posTags().toList\n",
    "\n",
    "    // Filter and return nouns\n",
    "    (words zip tags).filter( x => List(\"NN\",\"NNP\").contains(x._2))\n",
    "}).toList\n",
    "\n",
    "// Zip sentences together to get index\n",
    "val indexed_sentences = parsed_sentences.zipWithIndex\n",
    "\n",
    "// Parallize sentences\n",
    "val sentences_rdd = sc.parallelize(indexed_sentences)\n",
    "\n",
    "// Break into individual keywords\n",
    "case class SentenceKeyword(id: Int, keyword: String, pos: String)\n",
    "val keywords_by_sentence = sentences_rdd.flatMap(s => s._1.map(x => (s._2, x._1, x._2)))\n",
    "                                        .map(s => SentenceKeyword(s._1, s._2, s._3))\n",
    "\n",
    "// Save to disk\n",
    "keywords_by_sentence.toDF().write.parquet(\"file:///.../music/parsed_sentences/\")\n",
    "\n",
    "// Load from disk\n",
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "val keywords_by_sentence = sqlContext.load(\"file:///.../music/parsed_sentences/*\", \"parquet\").as[SentenceKeyword]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mSentenceKeyword\u001b[39m\n",
       "\u001b[36mkeywords_by_sentence\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mwrapper\u001b[39m.\u001b[32mwrapper\u001b[39m.\u001b[32mSentenceKeyword\u001b[39m] = [id: int, keyword: string ... 1 more field]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load from disk\n",
    "case class SentenceKeyword(id: Int, keyword: String, pos: String)\n",
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "val keywords_by_sentence = sqlContext.load(\"file:///home/garrett/dev/data/amazon/music/parsed_sentences/*\", \"parquet\").as[SentenceKeyword]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare word vectors\n",
    "In the reference papers for TextRank, they authors use a similarity metric between parts of the sentences that is essentially the intersection of common keywords / the union of all keywords between two sentences. In our model, we will take a different approach and create a more modern notion of comutational similarity amongst words in the music review corpus.\n",
    "\n",
    "To do this, we will first need to generate word vectors from the entire music review corpus (this will give us a mathematical representation of key concepts grounded in the reviewers' own language.\n",
    "\n",
    "We will use a separate, offline library called [FastText](https://github.com/facebookresearch/fastText) to generate the word vectors.\n",
    "\n",
    "Note: The below code will not run yet because of the Ivy model loading issue above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Save raw text file of all reviews -- one per line -- for FastText training purposes\n",
    "music_reviews.select(\"reviewText\").rdd.map(r => r(0).toString)\n",
    "                                  .coalesce(1).saveAsTextFile(\"file:///.../music/text\")\n",
    "\n",
    "/*\n",
    "    Train Word Vectors from Text -- Using Fast Text\n",
    "    =========\n",
    "    fasttext skipgram -input part-00000 -output word_vectors -dim 300\n",
    "*/\n",
    "\n",
    "// Load word-vectors into memory map\n",
    "val raw_word_vectors = sc.textFile(\"file:///.../music/text/word_vectors.vec\")\n",
    "                         .mapPartitionsWithIndex { (idx, iter) => \n",
    "                            if (idx == 0) iter.drop(1) else iter }\n",
    "\n",
    "// Get all keywords from parsed sentences\n",
    "val keywords = parsed_sentences.flatMap(x => x).map(x => x._1).toList\n",
    "\n",
    "// Filter word vectors\n",
    "val filtered_word_vectors = raw_word_vectors.filter(line => keywords.contains(line.split(\" \")(0)))\n",
    "filtered_word_vectors.cache\n",
    "\n",
    "// Merge with keywords\n",
    "val transformed_word_vectors = filtered_word_vectors.map(line => {\n",
    "    // Split line\n",
    "    val values = line.split(\" \")\n",
    "\n",
    "    // Add to in-memory word vector map\n",
    "    (values(0), values.slice(1, values.length).map(_.toFloat))\n",
    "})\n",
    "val grouped_keywords_vectors = keywords_by_sentence.map(sk => (sk.keyword, sk))\n",
    "                                                   .rdd.cogroup(transformed_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Map keywords to vectors\n",
    "case class MappedSentenceKeyword(id: Int, keyword: String, vector: Array[Float])\n",
    "val mapped_sentence_keywords = grouped_keywords_vectors.flatMap(grouped_keyword => {\n",
    "  // Check if word exists\n",
    "  if(grouped_keyword._2._2.toList.length > 0) { // word exists in vocabulary\n",
    "    // Get word vector\n",
    "    val word_vector = grouped_keyword._2._2.toList(0)\n",
    "\n",
    "    // Map each sentence keyword to vector\n",
    "    grouped_keyword._2._1.toList.map(sk => {\n",
    "        MappedSentenceKeyword(sk.id, sk.keyword, word_vector)\n",
    "    })\n",
    "  } else {\n",
    "    // Map each sentence keyword to vector\n",
    "    grouped_keyword._2._1.toList.map(sk => {\n",
    "        MappedSentenceKeyword(sk.id, sk.keyword, new Array[Float](300))\n",
    "    })\n",
    "  }\n",
    "})\n",
    "\n",
    "// Save to disk\n",
    "mapped_sentence_keywords.toDF().write.parquet(\"file:///.../music/mapped_sentence_keywords/\")\n",
    "\n",
    "// Load from disk\n",
    "val mapped_sentence_keywords = sqlContext.load(\"file:///.../music/mapped_sentence_keywords/*\",\n",
    "                                            \"parquet\").as[MappedSentenceKeyword]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sentence graph (in Spark GraphX)\n",
    "In order to leverage Spark's out-of-the-box PageRank computation, we need to load it into the necessary graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Load sentences\n",
    "case class IndexedSentence(id: Int, keywords: List[MappedSentenceKeyword])\n",
    "val indexed_sentences = mapped_sentence_keywords.rdd.map(sk => (sk.id, sk)).\n",
    "                            groupByKey().map(x => IndexedSentence(x._1, x._2.toList))\n",
    "\n",
    "// Create sentence pairs\n",
    "val sentence_pairs = indexed_sentences.flatMap(s1 => {\n",
    "  // Only create pairs for sentences that have an ID greater than the current sentences ID\n",
    "  sentences_array.slice(s1.id + 1, sentences_array.length).map(s2 => (s1, s2))\n",
    "})\n",
    "\n",
    "// Create sentence graph\n",
    "case class SentenceEdge(id_1: Int, id_2: Int, score: Double)\n",
    "val sentence_graph = sentence_pairs.map(S => {\n",
    "  // Zip keywords with vectors\n",
    "  val s1_vectors = (S._1.keywords.map(_.vector)).map(arr => new DenseVector(arr.map(_.toDouble)))\n",
    "  val s2_vectors = (S._2.keywords.map(_.vector)).map(arr => new DenseVector(arr.map(_.toDouble)))\n",
    "\n",
    "  // Fold and normalize each vector\n",
    "  val avg_s1_vector = s1_vectors.fold(DenseVector.zeros[Double](300))\n",
    "                                     ((acc,v) => { acc + v }) / (1.0 * s1_vectors.length)\n",
    "  val avg_s2_vector = s2_vectors.fold(DenseVector.zeros[Double](300))\n",
    "                                     ((acc,v) => { acc + v }) / (1.0 * s2_vectors.length)\n",
    "\n",
    "  // Return sentence graph edge\n",
    "  SentenceEdge(S._1.id, S._2.id, \n",
    "               CosineSimilarity.cosineSimilarity(avg_s1_vector.toArray, avg_s2_vector.toArray))\n",
    "})\n",
    "\n",
    "// Save sentence graph to disk\n",
    "sentence_graph.toDF().write.parquet(\"file:///.../music/sentence_graph/v1/\")\n",
    "\n",
    "// Load sentence graph from disk\n",
    "val sentence_graph = sqlContext.load(\"file:///.../music/sentence_graph/v1/*\").as[SentenceEdge]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute PageRank\n",
    "We're now going to derive a PageRank authority for each sentence Vertex in the sentence graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Create vertex RDD from sentences\n",
    "val sentenceVertices: RDD[(VertexId, String)] = \n",
    "    indexed_sentences.map(s => (s.id.toLong, s.id.toString))\n",
    "val defaultSentence = (\"-1\")\n",
    "\n",
    "// Create edges RDD from sentence graph -- only create links if above minimum similarity\n",
    "val sentenceEdges = sentence_graph.filter(se => se.score > 0.75).flatMap(se => {\n",
    "  List(Edge(se.id_1.toLong, se.id_2.toLong, se.score), \n",
    "       Edge(se.id_2.toLong, se.id_1.toLong, se.score))\n",
    "}).rdd\n",
    "\n",
    "// Create graph\n",
    "val graph = Graph(sentenceVertices, sentenceEdges, defaultSentence)\n",
    "graph.persist() // persist graph (for performance purposes\n",
    "\n",
    "// Calculate page rank\n",
    "val ranks = graph.pageRank(0.0001).vertices\n",
    "\n",
    "// Find top K sentences by rank\n",
    "val top_ranks = ranks.sortBy(_._2, ascending=false).take(10)\n",
    "val ranksAndSentences = ranks.join(sentenceVertices).sortBy(_._2._1, ascending=false).map(_._2._2)\n",
    "\n",
    "// Get the top 10 results\n",
    "ranksAndSentences.take(10)\n",
    "\n",
    "/*\n",
    "   Results: (1401, 824, 2360, 2717, 4322, 1150, 4363, 2320, 238, 3128)\n",
    " */\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get results\n",
    "Now that we have our ranked sentences, let's merge it with the sentences and print out the best ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Zip sentences together to get index\n",
    "case class SentenceRaw(id: Int, text: String)\n",
    "val indexed_sentences_original = sentences.toList\n",
    "                                          .zipWithIndex.map(x => (x._1.text(), \n",
    "                                                                  x._2))\n",
    "val sentencesArray = sc.parallelize(indexed_sentences_original).collect()\n",
    "sc.parallelize(sentencesArray.map(x => SentenceRaw(x._2, x._1)))\n",
    "  .toDF().registerTempTable(\"sentences\")\n",
    "\n",
    "// Show top sentences\n",
    "sqlContext.sql(\"SELECT text \n",
    "                FROM sentences \n",
    "                WHERE id in (1401, 824, 2360, 2717, 4322, \n",
    "                             1150, 4363, 2320, 238, 3128)\")\n",
    "           .map(r => r(0).toString).rdd.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results (Sample)\n",
    "\n",
    "> \"This is the best music, the best recording, the best rock album, the best concept album.\"\n",
    "\n",
    "> \"To make a long review short, you should buy \"Dark Side Of The Moon\" because: a) it's music, combining the band's sharp songwriting, outstanding musical chemistry, and impressive in-the-studio skills, is fantastic, b) it's timeless theme about all the things in life that can drive us mad---money, mortality, time (or lack of), war, etc., is pure genius, c) the clever lyrics by Roger Waters REALLY hit home, d) it's unsurpassed production & sound effects make it without question THE album to test your new stereo equipment with, and e) although I've never tried it myself, it's widely reputed to be a GREAT soundtrack album for....er, intimate encounters (especially while playing \"The Great Gig In The Sky\"---it's supposed to be really cool, man).\"\n",
    "\n",
    "> \"The Dark Side Of The Moon is a key album into defining the peak of space rock, the revival of psychedelic rock into modern settings, the point were blues is taken into a higher prospective, the point were progressive rock can't be called pretentious but remarkable nor snooze cultural but sincere and direct, and the unique characteristic where music fits with the listener and musicians in the most pure way; not their most complex neither their most cultural one, but the most pure.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
