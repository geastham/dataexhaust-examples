{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing Music Reviews with Graphical Models\n",
    "-------------------------------------------------\n",
    "\n",
    "## Overview\n",
    "Every day, digital shoppers across the globe generate hundreds of thousands of reviews on products - both new and old. As a digital retailer or brand, it is critical to understand not just the sentiment of this feedback but also the core concepts that customers write about; however, the pace of content generation has already outpaced the ability for marketing and merchandising teams at these organizations to read every piece of consumer generated content submitted.\n",
    "\n",
    "\n",
    "Dynamic content summarization techniques can provide a much-needed ability to programmatically identify core concepts within natural text and leverage this insight to condense large amounts of text into information-dense summarizations. In this talk, we will explore the current state-of-the-art in content summary by implementing the graph-based keyword extraction algorithm called TopicRank on music text reviews and then use these extracted concepts to summarize all of the reviews on a given album automatically.\n",
    "\n",
    "## Notebook Overview\n",
    "Below is a walkthrough from start to finish of a method for finding the top 10 most relevant sentences from a corpus of music reviews (in particular - we will be summarizing reviews for Pink Floyd's The Dark Side of the Moon).\n",
    "\n",
    "1. Load raw review data set (music reviews from Amazon - ~1m reviews)\n",
    "2. Find and parse sentences related to Dark Side of the Moon\n",
    "3. Prepare word embeddings from music corpus\n",
    "4. Create a sentence graph\n",
    "5. Compute PageRank on that graph\n",
    "6. Look at a few example results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$exclude.$                        , $ivy.$                            // for cleaner logs\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$profile.$           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // adjust spark version - spark >= 2.0\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
       "\n",
       "// General spark imports\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjupyter.spark.session._\n",
       "\n",
       "// Create sessions\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@73907edd\n",
       "\u001b[36msc\u001b[39m: \u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@d3a7cce\n",
       "\u001b[36msqlContext\u001b[39m: \u001b[32mSQLContext\u001b[39m = org.apache.spark.sql.SQLContext@172150da"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    " *  Environment Setup\n",
    " *  ========================\n",
    " *  - Jupyter-Scala (https://github.com/alexarchambault/jupyter-scala)\n",
    " */\n",
    "import $exclude.`org.slf4j:slf4j-log4j12`, $ivy.`org.slf4j:slf4j-nop:1.7.21` // for cleaner logs\n",
    "import $profile.`hadoop-2.6`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.1.0` // adjust spark version - spark >= 2.0\n",
    "import $ivy.`org.apache.hadoop:hadoop-aws:2.6.4`\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.0` // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
    "\n",
    "// General spark imports\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import jupyter.spark.session._\n",
    "\n",
    "// Create sessions\n",
    "val spark = JupyterSparkSession.builder() // important - call this rather than SparkSession.builder()\n",
    "  .jupyter() // this method must be called straightaway after builder()\n",
    "  // .yarn(\"/etc/hadoop/conf\") // optional, for Spark on YARN - argument is the Hadoop conf directory\n",
    "  // .emr(\"2.6.4\") // on AWS ElasticMapReduce, this adds aws-related to the spark jar list\n",
    "  .master(\"local[*]\") // change to \"yarn-client\" on YARN\n",
    "  .config(\"spark.driver.memory\", \"8g\")\n",
    "  .config(\"spark.executor.memory\", \"8g\")\n",
    "  .appName(\"Graph-based Review Summarization\")\n",
    "  .getOrCreate()\n",
    "\n",
    "// Access underlying spark context (for backwards compatibility)\n",
    "val sc = spark.sparkContext\n",
    "val sqlContext = spark.sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Load special ML / NLP libraries via interop\n",
    "interp.load.ivy(\"org.apache.spark\" %% \"spark-mllib\" % \"2.0.2\")\n",
    "interp.load.ivy(\"org.apache.spark\" %% \"spark-graphx\" % \"2.0.2\")\n",
    "interp.load.ivy(\"org.scalanlp\" %% \"breeze\" % \"0.13\")\n",
    "interp.load.ivy(\"edu.stanford.nlp\" % \"stanford-corenlp\" % \"3.6.0\")\n",
    "//interp.load.ivy(\"edu.stanford.nlp\" % \"stanford-corenlp\" % \"3.6.0\" classifier \"models\") -- throws error (https://github.com/alexarchambault/jupyter-scala/issues/128)\n",
    "interp.load.ivy(\"com.google.protobuf\" % \"protobuf-java\" % \"2.6.1\")\n",
    "\n",
    "// Spark SQL\n",
    "import sqlContext._\n",
    "import sqlContext.implicits._\n",
    "\n",
    "// ML imports\n",
    "import breeze.linalg._\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "// Graph imports\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.Dataset\n",
    "\n",
    "// NLP imports\n",
    "import edu.stanford.nlp.simple.Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and parse sentences\n",
    "We will be working with Amazon review data, made available by [UCSD](http://jmcauley.ucsd.edu/data/amazon/). We need to load it into a Spark dataframe, find the reviews related to our target Album (ASIN of B000000IRB), get the raw text and use StanfordNLP parser to get an enriched version of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd19.sc:5: Unable to find encoder for type stored in a Dataset.  Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._  Support for serializing other types will be added in future releases.\n",
      "val document = sqlContext.sql(\"SELECT reviewText FROM reviews WHERE asin = 'B000000IRB'\").map(r => r(0).toString).collect().mkString(\"\\n\\n\")\n",
      "                                                                                             ^cmd19.sc:8: not found: type Document\n",
      "val sentences = new Document(document).sentences() // 8077 sentences\n",
      "                    ^"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "// Load music reviews - find albums with most reviews\n",
    "val music_reviews = sqlContext.load(\"file:///home/garrett/dev/data/amazon/music/reviews_CDs_and_Vinyl_5.json\", \"json\")\n",
    "music_reviews.registerTempTable(\"reviews\")\n",
    "\n",
    "// Create merged review document for target albums\n",
    "val document = sqlContext.sql(\"SELECT reviewText FROM reviews WHERE asin = 'B000000IRB'\").map(r => r(0).toString).collect().mkString(\"\\n\\n\")\n",
    "\n",
    "// Get sentences\n",
    "val sentences = new Document(document).sentences() // 8077 sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres16\u001b[39m: \u001b[32mSQLContext\u001b[39m = org.apache.spark.sql.SQLContext@172150da"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
